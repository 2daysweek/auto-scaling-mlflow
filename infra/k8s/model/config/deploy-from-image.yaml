# https://kserve.github.io/website/master/get_started/first_isvc/#2-create-an-inferenceservice
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: custom-model
  namespace: model-serving
spec:
  predictor:
    containers:
      - image: luizvbo/example-mlflow-model:latest
        name: kserve-container
        resources:
          requests:
            cpu: 200m
            memory: 6Gi
          limits:
            cpu: 2
            memory: 6Gi
